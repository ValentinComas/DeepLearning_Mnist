{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRESSOT Arnaud 11505990\n",
    "# COMAS Valentin 11500223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import gzip # pour décompresser les données\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt # pour l'affichage\n",
    "import torch,torch.utils.data\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperation des données\n",
    "\n",
    "# nombre d'image lues à chaque fois dans la base d'apprentissage (laisser à 1 sauf pour la question optionnelle sur les minibatchs)\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "# on charge les données de la base MNIST\n",
    "data = pickle.load(gzip.open('mnist.pkl.gz'),encoding='latin1')\n",
    "# images de la base d'apprentissage\n",
    "train_data = data[0][0]\n",
    "# labels de la base d'apprentissage\n",
    "train_data_label = data[0][1]\n",
    "# images de la base de test\n",
    "test_data = data[1][0]\n",
    "# labels de la base de test\n",
    "test_data_label = data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight - Nb Step\n",
      "0.5 - 0.001  :  83.67142857142858 %\n",
      "0.7 - 0.001  :  83.67142857142858 %\n",
      "0.3 - 0.001  :  83.67142857142858 %\n",
      "-0.5 - 0.001  :  83.67142857142858 %\n",
      "-0.7 - 0.001  :  83.67142857142858 %\n",
      "-0.3 - 0.001  :  83.67142857142858 %\n",
      "0.5 - 0.01  :  75.55714285714285 %\n",
      "0.5 - 0.1  :  9.785714285714285 %\n",
      "0.5 - 0.005  :  80.58571428571429 %\n",
      "0.5 - 0.0001  :  84.82857142857144 %\n",
      "0.5 - 1e-05  :  79.98571428571428 %\n"
     ]
    }
   ],
   "source": [
    "# Partie 1 : Perceptron\n",
    "\n",
    "def test_perceptron(tests):\n",
    "    print(\"Weight - Nb Step\")\n",
    "    for i in test :\n",
    "        weight = i[0]\n",
    "        nb_step = i[1]\n",
    "        w = np.full((785, 10), weight)\n",
    "\n",
    "        for d in range(len(train_data)):\n",
    "            x = train_data[d]\n",
    "            x = np.append(x, 1)\n",
    "            y = np.dot(x, w)\n",
    "            dl = train_data_label[d]\n",
    "            a = nb_step * (dl - y)\n",
    "            b = np.outer(x, a)\n",
    "            w += b\n",
    "\n",
    "        nbCorrect = 0\n",
    "        for d in range(len(test_data)):\n",
    "            x = test_data[d]\n",
    "            x = np.append(x, 1)\n",
    "            y = np.dot(x, w)\n",
    "            index = np.argmax(test_data_label[d])\n",
    "            index2 = np.argmax(y)\n",
    "            if index == index2:\n",
    "                nbCorrect += 1\n",
    "        print(i[0],\"-\",i[1],\" : \",nbCorrect/len(test_data) * 100,\"%\")\n",
    "    \n",
    "\n",
    "\n",
    "test = [(0.5,0.001), (0.7,0.001), (0.3,0.001), (-0.5 ,0.001), (-0.7 ,0.001), (-0.3 ,0.001), (0.5,0.01), (0.5,0.1), (0.5,0.005), (0.5,0.0001), (0.5,0.00001)]\n",
    "test_perceptron(test)\n",
    "\n",
    "# On remarque que le poids initial n'est pas déterminant sur le résulat mais que c'est surtout le pas qui est important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de neuronnes cachés - Taille du pas - Poids initial couche cachée\n",
      "150 - 0.001 - 0.0  :  94.0 %\n",
      "100 - 0.001 - 0.0  :  93.8 %\n",
      "50 - 0.001 - 0.0  :  92.9 %\n",
      "200 - 0.001 - 0.0  :  94.38571428571429 %\n",
      "150 - 0.01 - 0.0  :  95.02857142857142 %\n",
      "150 - 0.1 - 0.0  :  58.94285714285714 %\n",
      "150 - 0.0001 - 0.0  :  83.81428571428572 %\n",
      "150 - 1e-05 - 0.0  :  81.24285714285713 %\n",
      "150 - 0.001 - 0.5  :  9.785714285714285 %\n",
      "150 - 0.001 - 0.8  :  9.785714285714285 %\n",
      "150 - 0.001 - -0.5  :  9.685714285714287 %\n",
      "150 - 0.001 - -0.8  :  9.785714285714285 %\n"
     ]
    }
   ],
   "source": [
    "# Partie 2 : Shallow network\n",
    "\n",
    "nb_hn = 150\n",
    "nb_step = 0.001\n",
    "\n",
    "def shallow_network(nb_hn, nb_step, weight):\n",
    "    w_h = np.full((np.shape(train_data)[1] + 1, nb_hn), weight)\n",
    "    w_o = np.random.randn(nb_hn, np.shape(train_data_label)[1])\n",
    "    for i in range(5):\n",
    "        for d in range(len(train_data)):\n",
    "            # Choisir entrée\n",
    "            x = train_data[d]\n",
    "            x = np.append(x, 1.)\n",
    "\n",
    "            # Propagation yi(1)\n",
    "            y_h = 1. / (1. + np.exp(-np.dot(x, w_h)))\n",
    "\n",
    "            # Propagation yi(2)\n",
    "            y_o = np.dot(y_h, w_o)\n",
    "\n",
    "            # Rétro Propagation \n",
    "\n",
    "            # Calcul de l'erreur\n",
    "            dl = train_data_label[d]\n",
    "            m_o = dl - y_o\n",
    "\n",
    "            # Rétro Propa de l'erreur\n",
    "            a1 = y_h * (1 - y_h)\n",
    "            a2 = np.dot(w_o, m_o)\n",
    "            m_h = a1 * a2\n",
    "\n",
    "            # Modification poids\n",
    "            dw_o = nb_step * np.outer(y_h, m_o)\n",
    "            w_o += dw_o\n",
    "            dw_h = nb_step * np.outer(x, m_h)\n",
    "            w_h += dw_h\n",
    "            \n",
    "    nbCorrect = 0\n",
    "    for d in range(len(test_data)):\n",
    "        x = test_data[d]\n",
    "        x = np.append(x, 1)\n",
    "\n",
    "        y_h = 1. / (1. + np.exp(-np.dot(x, w_h)))\n",
    "\n",
    "        y_o = np.dot(y_h, w_o)\n",
    "        index = np.argmax(test_data_label[d])\n",
    "        index2 = np.argmax(y_o)\n",
    "        if index == index2:\n",
    "            nbCorrect += 1\n",
    "    return nbCorrect/len(test_data) * 100\n",
    "\n",
    "def test_shallow_network(tests):\n",
    "    print(\"Nb de neuronnes cachés - Taille du pas - Poids initial couche cachée\")\n",
    "    for i in tests:\n",
    "        result = shallow_network(i[0],i[1],i[2])\n",
    "        print(i[0],\"-\",i[1],\"-\",i[2],\" : \",result,\"%\")\n",
    "    \n",
    "tests = [(150, 0.001, 0.),(100, 0.001, 0.),(50, 0.001, 0.),(200, 0.001, 0.),(150, 0.01, 0.),(150, 0.1, 0.),(150, 0.0001, 0.),(150, 0.00001, 0.), (150, 0.001, 0.5),(150, 0.001, 0.8),(150, 0.001, -0.5),(150, 0.001, -0.8)]\n",
    "test_shallow_network(tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de pas - Epochs - Layers_size - Fonction\n",
      "0.001 - 6 - [784, 150, 30, 10] - Sigmoid()  : Average loss :  0.005562063564721549  Accuracy :  96.65714285714286 %\n",
      "0.01 - 6 - [784, 150, 30, 10] - Sigmoid()  : Average loss :  0.006332226827426504  Accuracy :  96.24285714285715 %\n",
      "0.0001 - 6 - [784, 150, 30, 10] - Sigmoid()  : Average loss :  0.050461277777861274  Accuracy :  69.75714285714285 %\n",
      "0.001 - 4 - [784, 150, 30, 10] - Sigmoid()  : Average loss :  0.0075352249823294444  Accuracy :  95.6 %\n",
      "0.001 - 8 - [784, 150, 30, 10] - Sigmoid()  : Average loss :  0.0046503896221581405  Accuracy :  97.07142857142857 %\n",
      "0.001 - 6 - [784, 150, 30, 10] - Sigmoid()  : Average loss :  0.005772379419083986  Accuracy :  96.5 %\n",
      "0.001 - 6 - [784, 10] - Sigmoid()  : Average loss :  0.016524196397277625  Accuracy :  91.6 %\n",
      "0.001 - 6 - [784, 150, 100, 50, 10] - Sigmoid()  : Average loss :  0.005506017547457824  Accuracy :  96.38571428571429 %\n",
      "0.001 - 6 - [784, 300, 150, 30, 10] - Sigmoid()  : Average loss :  0.0051987191126058215  Accuracy :  96.71428571428571 %\n",
      "0.001 - 6 - [784, 150, 30, 10] - ReLU()  : Average loss :  0.04254201770604294  Accuracy :  58.97142857142857 %\n",
      "0.01 - 6 - [784, 150, 30, 10] - ReLU()  : Average loss :  0.0718865278611287  Accuracy :  29.2 %\n",
      "0.0001 - 6 - [784, 150, 30, 10] - ReLU()  : Average loss :  0.026127668240123282  Accuracy :  78.7 %\n",
      "0.001 - 4 - [784, 150, 30, 10] - ReLU()  : Average loss :  0.023483827432241422  Accuracy :  78.65714285714286 %\n",
      "0.001 - 8 - [784, 150, 30, 10] - ReLU()  : Average loss :  0.023646770854609298  Accuracy :  77.55714285714286 %\n",
      "0.001 - 6 - [784, 150, 30, 10] - ReLU()  : Average loss :  0.0414598409184784  Accuracy :  59.885714285714286 %\n",
      "0.001 - 6 - [784, 10] - ReLU()  : Average loss :  0.04408361493805394  Accuracy :  64.55714285714286 %\n",
      "0.001 - 6 - [784, 300, 150, 30, 10] - ReLU()  : Average loss :  0.023610640567782372  Accuracy :  77.58571428571429 %\n"
     ]
    }
   ],
   "source": [
    "#  Partie 3 : Deep network\n",
    "\n",
    "# Import as Tensors\n",
    "\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "\n",
    "data = pickle.load(gzip.open('mnist.pkl.gz'),encoding='latin1')\n",
    "train_data = torch.Tensor(data[0][0])\n",
    "train_data_label = torch.Tensor(data[0][1])\n",
    "test_data = torch.Tensor(data[1][0])\n",
    "test_data_label = torch.Tensor(data[1][1])\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data,train_data_label)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data,test_data_label)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, layers_size, activation_function):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.activation_function = activation_function\n",
    "        self.layers = nn.ModuleList()\n",
    "        #create the layers\n",
    "        for i in range(len(layers_size) - 1):\n",
    "            layer = nn.Linear(layers_size[i], layers_size[i+1])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = self.activation_function(l(x))\n",
    "        return x\n",
    "\n",
    "def instance_DeepNN(nb_step, epochs, layers_size, activation_function):\n",
    "    deepNN = DeepNN(layers_size, activation_function)\n",
    "    optimizer = torch.optim.Adam(deepNN.parameters(), lr=nb_step)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        deepNN.train()\n",
    "        for batch_ep, (image,label) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = deepNN(image)\n",
    "            loss_f = nn.MSELoss()\n",
    "            loss = loss_f(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    deepNN.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            output = deepNN(image)\n",
    "            lose_f = nn.MSELoss()\n",
    "            test_loss += lose_f(output, label).item() \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += int(np.argmax(output)) == int(np.argmax(label))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    return (test_loss, 100. * correct / len(test_loader.dataset))\n",
    "            \n",
    "def test_DeepNN(tests):\n",
    "    print(\"Nb de pas - Epochs - Layers_size - Fonction\")\n",
    "    for i in tests:\n",
    "        test_loss, percent = instance_DeepNN(i[0], i[1], i[2], i[3])\n",
    "        print(i[0],\"-\",i[1],\"-\",i[2],\"-\",i[3],\" : Average loss : \",test_loss,\" Accuracy : \",percent,\"%\")\n",
    "\n",
    "tests = [(0.001, 6, [784, 150 ,30, 10], nn.Sigmoid()), (0.01, 6, [784, 150 ,30, 10], nn.Sigmoid()), (0.0001, 6, [784, 150 ,30, 10], nn.Sigmoid()), (0.001, 4, [784, 150 ,30, 10], nn.Sigmoid()), (0.001, 8, [784, 150 ,30, 10], nn.Sigmoid()), (0.001, 6, [784, 150 ,30, 10], nn.Sigmoid()), (0.001, 6, [784, 10], nn.Sigmoid()),(0.001, 6, [784, 150 ,100, 50, 10], nn.Sigmoid()),(0.001, 6, [784, 300, 150 ,30, 10], nn.Sigmoid()), (0.001, 6, [784, 150 ,30, 10], nn.ReLU()), (0.01, 6, [784, 150 ,30, 10], nn.ReLU()), (0.0001, 6, [784, 150 ,30, 10], nn.ReLU()), (0.001, 4, [784, 150 ,30, 10], nn.ReLU()), (0.001, 8, [784, 150 ,30, 10], nn.ReLU()), (0.001, 6, [784, 150 ,30, 10], nn.ReLU()), (0.001, 6, [784, 10], nn.ReLU()),(0.001, 6, [784, 300, 150 ,30, 10], nn.ReLU())]\n",
    "\n",
    "\n",
    "test_DeepNN(tests)\n",
    "\n",
    "# On trouve le meilleur résulat avec 0.001 - 8 - [784, 150, 30, 10] - Sigmoid().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0777, Accuracy: 9749/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Partie 4 : Pour aller plus loin\n",
    "\n",
    "# another load to allow transformations\n",
    "\n",
    "import torchvision\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('Mnist', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()\n",
    "                             ])),\n",
    "  batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('Mnist', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()\n",
    "                             ])),\n",
    "  batch_size=1, shuffle=True)\n",
    "class Conv2DNN(nn.Module):\n",
    "    def __init__(self, activation_function):\n",
    "        super(Conv2DNN, self).__init__()\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        #create the layers with a dropout of 0.25 for each convolutionnal layer to prevent overfitting\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.lin = nn.Linear(64 * 5 * 5, 64)\n",
    "        self.lin2 = nn.Linear(64, 10)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.activation_function(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.activation_function(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.lin(x)\n",
    "        x = self.activation_function(x)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "            \n",
    "nb_step = 0.001\n",
    "epochs = 5\n",
    "activation_function = nn.Sigmoid()\n",
    "\n",
    "log = True\n",
    "\n",
    "conv2dNN = Conv2DNN(activation_function)\n",
    "optimizer = torch.optim.Adam(conv2dNN.parameters(), lr=nb_step)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    conv2dNN.train()\n",
    "    for batch_ep, (image,label) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = conv2dNN(image)\n",
    "        loss_f = nn.CrossEntropyLoss()\n",
    "        loss = loss_f(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "conv2dNN.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for image, label in test_loader:\n",
    "        output = conv2dNN(image)\n",
    "        lose_f = nn.CrossEntropyLoss()\n",
    "        test_loss += lose_f(output, label).item() \n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += np.argmax(output) == label[0]\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "# Après plusieurs tests, nous en avons conclu que ce sont les meilleurs parametres pour ce réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
